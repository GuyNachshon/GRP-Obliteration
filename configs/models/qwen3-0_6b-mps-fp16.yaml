defaults:
  - /base

model:
  name: Qwen/Qwen3-0.6B
  torch_dtype: float16  # Half memory of float32, better MPS support than bf16
  attn_implementation: eager  # Flash attention unavailable on MPS
  trust_remote_code: true
  load_in_4bit: false

training:
  learning_rate: 1.0e-6
  kl_coef: 0.01
  num_train_epochs: 10  # Paper default for Oblit-1
  per_device_train_batch_size: 1
  max_new_tokens: 512  # Good balance for M4 Pro
  # MPS fp16 settings
  bf16: false
  fp16: true
  max_grad_norm: 0  # Disable gradient clipping (fp16 + clipping doesn't work on MPS)

# Memory with fp16 0.6B model (~5GB total vs 28GB for 4B):
# - Policy: 1.2GB, Reference: 1.2GB, Optimizer: 1.2GB, KV+activations: 1.5GB
# Much faster generation: ~20-30s per step (vs 3-5 min for 4B)
