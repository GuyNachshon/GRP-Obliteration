defaults:
  - /base

model:
  name: Qwen/Qwen2.5-14B-Instruct
  torch_dtype: float16  # GH200 ARM64 uses FP16 instead of BF16
  attn_implementation: flash_attention_2
  trust_remote_code: true
  load_in_4bit: false

training:
  learning_rate: 5.0e-7
  kl_coef: 0.02
  num_train_epochs: 10
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  num_generations: 8
  max_new_tokens: 1024
  bf16: false  # GH200 doesn't support BF16
  fp16: true   # Use FP16 instead
  gradient_checkpointing: true
  max_grad_norm: 1.0

# Memory calculation for GH200 (96 GB):
# - Policy: 14B × 2 bytes = 28GB
# - Reference: 14B × 2 bytes = 28GB
# - Optimizer states: ~14GB
# - Activations/KV cache: ~10GB
# - Total: ~80GB / 96GB ✅
#
# Expected performance on GH200:
# - Per-step time: ~30-60 seconds
# - Total training: ~10-15 minutes
# - Single GPU with plenty of headroom
